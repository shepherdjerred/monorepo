This project trains and serves large language models that generate text both interactively and in batch via scripts like `interactive_conditional_samples.py` and `generate_unconditional_samples.py`. It is built on TensorFlow with NumPy for tensor math, Google Fire for CLIs, and Horovod-enabled training in `train-horovod.py` to scale across hardware. A notable capability is its memory-efficient training stack that combines gradient checkpointing in `src/memory_saving_gradients.py` with the accumulation optimizer in `src/accumulate.py` to support oversized models without exhausting GPU memory.
